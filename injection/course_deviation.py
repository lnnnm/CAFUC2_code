import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json
import math

# ========== 1. é…ç½® ==========
INPUT_FOLDERS = [
    "clean_data/SR20G6"
    # "normal_data/C172S",
    # # "processed_data/DA42NG",
    # "normal_data/SR20",
    # "normal_data/SR20G6",
]

OUTPUT_ROOT = Path("course_deviation/clean_data_SR20G6")
OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

# --- å¼‚å¸¸æ³¨å…¥å‚æ•° ---
TARGET_RATIO = 0.03  # å¼‚å¸¸æ¯”ä¾‹
WINDOW_SIZE = 30  # æ¯æ®µå¼‚å¸¸é•¿åº¦ï¼ˆç§’ï¼‰

# --- æ–°å¢ï¼šç‰©ç†æ¨¡å‹å‚æ•° ---
# å‡è®¾ä¸€ä¸ªåˆç†çš„å¹³å‡åœ°é€Ÿï¼ˆå•ä½ï¼šèŠ‚ knotsï¼‰ï¼Œå› ä¸ºåŸå§‹æ•°æ®æ²¡æœ‰æä¾›
# 120èŠ‚å¯¹äºC172/SR20ç­‰è®­ç»ƒé£æœºæ˜¯ä¸€ä¸ªå¸¸è§çš„å·¡èˆªé€Ÿåº¦
ASSUMED_GROUND_SPEED_KNOTS = 120.0
# å‡è®¾æ•°æ®çš„é‡‡æ ·é—´éš”ä¸º1ç§’
SAMPLING_INTERVAL_SECONDS = 1.0


# ========== 2. è¾…åŠ©å‡½æ•°ï¼šèˆªä½æ¨ç®— ==========
def calculate_new_position(lat_deg, lon_deg, bearing_deg, distance_m):
    """
    æ ¹æ®èµ·ç‚¹ã€æ–¹ä½è§’å’Œè·ç¦»ï¼Œè®¡ç®—æ–°çš„ç»çº¬åº¦åæ ‡ã€‚
    ä½¿ç”¨çƒé¢æ¨¡å‹è¿›è¡Œä¼°ç®—ã€‚
    """
    R = 6378137.0  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰

    lat_rad = math.radians(lat_deg)
    lon_rad = math.radians(lon_deg)
    bearing_rad = math.radians(bearing_deg)

    dist_rad = distance_m / R  # è§’è·ç¦»

    new_lat_rad = math.asin(math.sin(lat_rad) * math.cos(dist_rad) +
                            math.cos(lat_rad) * math.sin(dist_rad) * math.cos(bearing_rad))

    new_lon_rad = lon_rad + math.atan2(math.sin(bearing_rad) * math.sin(dist_rad) * math.cos(lat_rad),
                                       math.cos(dist_rad) - math.sin(lat_rad) * math.sin(new_lat_rad))

    return math.degrees(new_lat_rad), math.degrees(new_lon_rad)


# ========== 3. æ ¸å¿ƒæ³¨å…¥å‡½æ•° ==========
def inject_realistic_course_deviation(df: pd.DataFrame, num_windows: int):
    """
    åœ¨ DataFrame ä¸­æ³¨å…¥ç‰©ç†ä¸Šåˆç†çš„èˆªé“åç§»å¼‚å¸¸ã€‚
    æ ‡ç­¾ç­–ç•¥å·²ä¿®æ”¹ä¸ºï¼šå¼‚å¸¸è¡Œä¸ºæ ‡è®°ä¸º1ï¼Œæ­£å¸¸è¡Œä¸ºæ ‡è®°ä¸º0ã€‚
    """
    df = df.copy()
    df.columns = df.columns.str.strip()

    if "label" not in df.columns:
        df["label"] = 0
    # ç¡®ä¿åˆ—æ˜¯æ•´æ•°ç±»å‹ï¼Œå¹¶å°†å¯èƒ½å­˜åœ¨çš„ç©ºå€¼å¡«å……ä¸º0
    df["label"] = df["label"].fillna(0).astype(int)

    # é¢„å¤„ç†æ•°å€¼åˆ—
    numeric_cols = ['Latitude', 'Longitude', 'TRK', 'HDG', 'WptDst']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    df = df.dropna(subset=[c for c in numeric_cols if c in df.columns]).reset_index(drop=True)
    n = len(df)

    anomaly_info_list = []
    if n <= WINDOW_SIZE * 2 or num_windows == 0:
        return df, anomaly_info_list

    distance_per_step_m = (ASSUMED_GROUND_SPEED_KNOTS * 0.514444) * SAMPLING_INTERVAL_SECONDS

    processed_indices = set()
    for _ in range(num_windows):
        start = np.random.randint(0, n - WINDOW_SIZE)
        end = start + WINDOW_SIZE

        if any(i in processed_indices for i in range(start, end + 1)):
            continue

        affected_cols = []
        trk_offset = np.random.uniform(15, 30) * np.random.choice([-1, 1])
        hdg_offset = trk_offset + np.random.uniform(-2, 2)

        if 'TRK' in df.columns:
            df.loc[start:end, 'TRK'] += trk_offset
            affected_cols.append(df.columns.get_loc('TRK'))
        if 'HDG' in df.columns:
            df.loc[start:end, 'HDG'] += hdg_offset
            affected_cols.append(df.columns.get_loc('HDG'))

        if 'Latitude' in df.columns and 'Longitude' in df.columns and 'TRK' in df.columns:
            for i in range(start, end + 1):
                prev_lat = df.loc[i - 1, 'Latitude']
                prev_lon = df.loc[i - 1, 'Longitude']
                current_bearing = df.loc[i, 'TRK']
                new_lat, new_lon = calculate_new_position(prev_lat, prev_lon, current_bearing, distance_per_step_m)
                df.loc[i, 'Latitude'] = new_lat
                df.loc[i, 'Longitude'] = new_lon
            affected_cols.extend([df.columns.get_loc('Latitude'), df.columns.get_loc('Longitude')])

        if 'WptDst' in df.columns:
            dist_increase = np.linspace(0, np.random.uniform(500, 1500), WINDOW_SIZE + 1)
            df.loc[start:end, 'WptDst'] += dist_increase
            affected_cols.append(df.columns.get_loc('WptDst'))

        # --- ä¿®æ”¹ç‚¹2ï¼šå°†æ•´ä¸ªå¼‚å¸¸æ®µè½çš„æ ‡ç­¾è®¾ç½®ä¸º 1 ---
        df.loc[start:end, 'label'] = 1

        anomaly_info_list.append({
            "start_idx": int(start), "end_idx": int(end), "affected_cols": sorted(list(set(affected_cols)))
        })
        processed_indices.update(range(start, end + 1))

    return df, anomaly_info_list


# ========== 4. æ‰¹é‡å¤„ç† ==========
anomaly_meta = []
file_counter = 1  # åˆå§‹åŒ–æ–‡ä»¶è®¡æ•°å™¨

for folder in INPUT_FOLDERS:
    input_dir = Path(folder)
    plane_type = input_dir.name
    print(f"ğŸ“‚ æ­£åœ¨å¤„ç†: {plane_type}")

    for file in tqdm(list(input_dir.glob("*.csv"))):
        try:
            df = pd.read_csv(file, low_memory=False)
            if not any(col.strip() in df.columns for col in ["Latitude", "Longitude", "TRK"]):
                print(f"âš ï¸ è·³è¿‡ç¼ºå°‘å…³é”®å¯¼èˆªåˆ—çš„æ–‡ä»¶: {file.name}")
                continue

            n = len(df)
            num_windows = max(1, int((TARGET_RATIO * n) / WINDOW_SIZE))

            abnormal_df, info_list = inject_realistic_course_deviation(df, num_windows)

            out_path = OUTPUT_ROOT / f"{file_counter}.csv"
            abnormal_df.to_csv(out_path, index=False)

            for info in info_list:
                anomaly_meta.append({
                    "file": f"{file_counter}.csv",
                    "original_file": file.name,
                    "plane_type": plane_type,
                    **info
                })

            file_counter += 1  # è®¡æ•°å™¨è‡ªå¢
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file.name} å¤±è´¥: {e}")
            continue

# ========== 5. ä¿å­˜å…ƒæ•°æ® ==========
if anomaly_meta:
    meta_path = OUTPUT_ROOT / "anomaly_segments.csv"
    pd.DataFrame(anomaly_meta).to_csv(meta_path, index=False)

    config = {
        "target_ratio": TARGET_RATIO,
        "window_size": WINDOW_SIZE,
        "assumed_ground_speed_knots": ASSUMED_GROUND_SPEED_KNOTS,
        "sampling_interval_seconds": SAMPLING_INTERVAL_SECONDS,
        "total_segments": len(anomaly_meta)
    }
    with open(OUTPUT_ROOT / "config.json", "w", encoding="utf-8") as fp:
        json.dump(config, fp, indent=2)

    print(f"\nğŸ“Š å¼‚å¸¸å…ƒæ•°æ®ä¿å­˜è‡³: {meta_path}")
    print(f"ğŸ“„ é…ç½®ä¿¡æ¯ä¿å­˜è‡³: {OUTPUT_ROOT / 'config.json'}")
else:
    print("\nâš ï¸ æœªç”Ÿæˆä»»ä½•å¼‚å¸¸æ•°æ®")

print("\nğŸ¯ ç‰©ç†ä¸Šåˆç†çš„èˆªé“åç§»å¼‚å¸¸æ³¨å…¥å®Œæˆï¼")